# AI vs. Human Voice Classification Using Wav2Vec2

## Overview

This project provides an end-to-end solution for classifying audio into **human** or **AI-generated** categories using the **Wav2Vec2** model. It supports multi-class classification and distinguishes between real voices and synthetic audio generated by models like **DiffWave**, **WaveNet**, and more. The pipeline includes:

- **Dataset Preparation**: Combine human and AI-generated audio into a structured, multi-class dataset.
- **Preprocessing**: Resample audio, normalize lengths, and generate attention masks for training.
- **Fine-Tuning**: Adapt Wav2Vec2 for custom classification tasks.
- **Inference**: Classify unseen audio with confidence scores.
- **API Deployment**: Real-time predictions via FastAPI.

---

## Key Features

- **Multi-Class Audio Classification**: Supports detection of human voices and six AI-generated classes (DiffWave, WaveNet, etc.).
- **Optimized for Performance**: Includes techniques like attention masks, quantization, and ONNX conversion for efficiency.
- **Deployment-Ready**: Real-time audio classification using FastAPI.

## Dataset

### **Source**
The dataset is derived from the **LibriSeVoc** corpus, which includes real human audio samples and synthetic speech generated by:
- **DiffWave**
- **WaveNet**
- **MelGAN**
- **Parallel WaveGAN**
- **WaveGrad**
- **WaveRNN**

### **Structure**
- Real human voices are stored in the `gt` folder.
- AI-generated voices are organized into separate folders (`diffwave`, `wavenet`, etc.).
- Dataset includes **13,201 samples** per class, ensuring balance.

## File Structure
* ├── data_preparation.py # Combine raw audio into a CSV dataset 
* ├── preprocess_dataset.py # Resample, pad, and preprocess audio 
* ├── train.py # Fine-tune Wav2Vec2 on the dataset 
* ├── inference.py # Perform inference on unseen audio 
* ├── app.py # FastAPI app for real-time classification 
* ├── README.md # Documentation

  ## Setup Instructions

### **Step 1: Clone Repository**
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```
### **Step 2: Install Dependencies**
```bash
pip install -r requirements.txt
```
### **Step 3: Dataset Preparation**
## Run data_preparation.py to prepare a multi-class dataset:

```bash
python data_preparation.py
```
## This script combines real and AI-generated audio into a structured CSV file.

### **Step 4: Preprocessing**
Run preprocess_dataset.py to resample, pad, and preprocess the dataset:

```bash
python preprocess_dataset.py
```
### **Step 5: Fine-Tuning**
Fine-tune the Wav2Vec2 model using train.py:

```bash
python train.py
```
### **Step 6: Inference**
Use inference.py to classify unseen audio:

```bash
python inference.py --audio_file path_to_audio.wav
```
### **Step 7: Deploy API**
## Run app.py to start the FastAPI server:

```bash
uvicorn app:app --reload
```
## Access the API at http://127.0.0.1:8000/predict.

# Technical Details
## Model
### The project uses Wav2Vec2ForSequenceClassification fine-tuned on a multi-class dataset of real and AI-generated voices.
## Preprocessing
* Audio is resampled to 16kHz.
* Attention masks and padding are applied for consistent input length.
## Fine-Tuning
The model is fine-tuned for 10 epochs with a learning rate of 2e-5.
Techniques like gradient checkpointing and mixed precision are enabled to improve efficiency.
## Inference
Inference is optimized with ONNX quantization for deployment on edge devices.
## Results
* Validation Accuracy: Achieved 99.8% accuracy on the test set.
* Robustness: The model generalizes well across synthetic and human voices.
## Future Improvements
- Add support for more AI-generated voice datasets.
- Test inference on mobile devices using ONNX.
- Implement real-time streaming support for API.
## License
This project is licensed under the MIT License.



